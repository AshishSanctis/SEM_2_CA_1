{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b91f7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/hduser/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, explode\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.types import DoubleType, ArrayType, StringType\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import col, udf\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.sql.types import DoubleType, ArrayType, StringType\n",
    "from pyspark.sql.types import DoubleType\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.functions import explode\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30695fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/04 18:33:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Importing PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e130da56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ubuntu-22.04-arm64.shared:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425efe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "data_df = spark.read.load(\"hdfs://localhost:9000/user1/train.csv\", format=\"csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4bc5638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|class_index|        review_title|         review_text|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          3|  more like funchuck|\"Gave this to my ...|\n",
      "|          5|           Inspiring|I hope a lot of p...|\n",
      "|          5|The best soundtra...|I'm reading a lot...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "379101b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:==============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 3000000\n",
      "Column names: ['class_index', 'review_title', 'review_text']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check the number of rows\n",
    "num_rows = data_df.count()\n",
    "print(\"Number of rows:\", num_rows)\n",
    "\n",
    "# Check the list of column names\n",
    "column_names = data_df.columns\n",
    "print(\"Column names:\", column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1ec3fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "root\n",
      " |-- class_index: string (nullable = true)\n",
      " |-- review_title: string (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing Schema\n",
    "print(\"Schema:\")\n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f45ffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|class_index|        review_title|         review_text|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          3|  more like funchuck|\"Gave this to my ...|\n",
      "|          5|           Inspiring|I hope a lot of p...|\n",
      "|          5|The best soundtra...|I'm reading a lot...|\n",
      "|          4|    Chrono Cross OST|\"The music of Yas...|\n",
      "|          5| Too good to be true|Probably the grea...|\n",
      "|          5|There's a reason ...|There's a reason ...|\n",
      "|          1|        Buyer beware|\"This is a self-p...|\n",
      "|          4|Errors, but great...|I was a dissapoin...|\n",
      "|          1|          The Worst!|A complete waste ...|\n",
      "|          1|           Oh please|I guess you have ...|\n",
      "|          1|Awful beyond belief!|\"I feel I have to...|\n",
      "|          4|A romantic zen ba...|\"When you hear fo...|\n",
      "|          5|Lower leg comfort...|Excellent stockin...|\n",
      "|          3|Delivery was very...|It took almost 3 ...|\n",
      "|          2|sizes recomended ...|sizes are much sm...|\n",
      "|          3|            Overbury|Full of intrigue ...|\n",
      "|          1|Another Abysmal D...|\"Rather than scra...|\n",
      "|          4|Wardell's book is...|\"Steven Wardell's...|\n",
      "|          4|i liked this albu...|\"I heard a song o...|\n",
      "|          3|Better than I tho...|I wrote a harsh r...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show sample rows after removing null values\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a54a2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'class_index' column and perform aggregation\n",
    "aggregated_df = data_df.groupBy(\"class_index\").agg(\n",
    "    F.count(\"*\").alias(\"count_reviews\"),\n",
    "    F.avg(\"class_index\").alias(\"avg_class_index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3538bf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:==============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------+\n",
      "|class_index|count_reviews|avg_class_index|\n",
      "+-----------+-------------+---------------+\n",
      "|          3|       600000|            3.0|\n",
      "|          5|       600000|            5.0|\n",
      "|          1|       600000|            1.0|\n",
      "|          4|       600000|            4.0|\n",
      "|          2|       600000|            2.0|\n",
      "+-----------+-------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the aggregated DataFrame\n",
    "aggregated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac64acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the DataFrame by a single column\n",
    "sorted_df = data_df.orderBy(\"class_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b335531b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:==============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|class_index|        review_title|         review_text|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          1|  Read Dante Instead|Reading this book...|\n",
      "|          1|         Didn't work|This tape was imm...|\n",
      "|          1|        unhappy feet|A disappointment ...|\n",
      "|          1|                Poor|\"This is actually...|\n",
      "|          1|Really? Magic gecko?|Right about the t...|\n",
      "|          1|             Useless|I don't think it ...|\n",
      "|          1|             Bad Buy|I bought this rin...|\n",
      "|          1|  Poor Sound Quality|The sound quality...|\n",
      "|          1|    Frustrating game|The good news is ...|\n",
      "|          1|     Waste of money.|This thing is a p...|\n",
      "|          1|     A Steaming Pile|\"This album is ho...|\n",
      "|          1|          Wrong item|Ordered the Belki...|\n",
      "|          1|     TOTALLY USELESS|\"Not only is the ...|\n",
      "|          1|Product did not work|This didn't work ...|\n",
      "|          1|Statistics for st...|This book will pr...|\n",
      "|          1|Auto reverses and...|The sound quality...|\n",
      "|          1|Shouldn't be adve...|I gave this toy t...|\n",
      "|          1|Has a loud clicki...|Makes a gear grid...|\n",
      "|          1|    Absolutely Awful|This product does...|\n",
      "|          1|           Defective|I purchased this ...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Showing the sorted DataFrame\n",
    "sorted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deede37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|class_index|        review_title|         review_text|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          5|For those of us w...|\"Louis adventures...|\n",
      "|          5|   Coolest game eva!|I love the nancy ...|\n",
      "|          5|    Soft and Cuddly!|My son loves his ...|\n",
      "|          5|      Kitchen Review|My husband and I ...|\n",
      "|          5|      The New Empire|From the other st...|\n",
      "|          5|Kind of creepy bu...|\"In the begining ...|\n",
      "|          5|A Reference Recor...|In the mid '70s, ...|\n",
      "|          5|great product at ...|I have purchased ...|\n",
      "|          5|           excellent|Exactly what I no...|\n",
      "|          5|          Great pan!|This is a great q...|\n",
      "|          5|Everything a roma...|Just finished thi...|\n",
      "|          5|this is the one y...|best value in a t...|\n",
      "|          5|Ice cream scoop, ...|Has been operatin...|\n",
      "|          5|  Norpro popover pan|We had popovers b...|\n",
      "|          5|       giving it all|Real life lyrics ...|\n",
      "|          5|works perfect in ...|bought this and w...|\n",
      "|          5|On time and what ...|See above. I am w...|\n",
      "|          5|             Punchy!|Can't believe I h...|\n",
      "|          5|    Wow! Hot hot hot|Jessie is a stron...|\n",
      "|          5|Great gear, impre...|As good as a Colu...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Sort in descending order, using desc() function\n",
    "sorted_desc_df = data_df.orderBy(data_df[\"class_index\"].desc())\n",
    "sorted_desc_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8890a6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|class_index|        review_title|         review_text|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          1|                null|\"Unless you're a ...|\n",
      "|          1|                null|\"I was extremely ...|\n",
      "|          1|                null|Just wanted to vo...|\n",
      "|          1|                null|Can't write a rev...|\n",
      "|          1|                null|These guys sure h...|\n",
      "|          1|                null|I didn't really c...|\n",
      "|          1|                null|\"Some good rockin...|\n",
      "|          1|                null|This is lightweig...|\n",
      "|          1|                null|\"I remember order...|\n",
      "|          1|                null|And here we have ...|\n",
      "|          1|                null|I ordered a 12 fo...|\n",
      "|          1|                null|The battery was p...|\n",
      "|          1|                   !|\"This book is com...|\n",
      "|          1|! - STAY AWAY! WO...|Please for the lo...|\n",
      "|          1|! CAUTION - NOT T...|!! CAUTION!!Re-re...|\n",
      "|          1|! DANGER! This fi...|This is an excell...|\n",
      "|          1|! LONG LIVE THE F...|\"Uau, what a bad ...|\n",
      "|          1|! SOUNDS LIKE HE ...|Busta rhymes is a...|\n",
      "|          1|! WARNING - DO NO...|\"I purchased my D...|\n",
      "|          1|! WARNING ... WAR...|Warning, the game...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Sorting by multiple columns\n",
    "multi_sorted_df = data_df.orderBy(\"class_index\", \"review_title\")\n",
    "multi_sorted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c152b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering rows where class_index is equal to 5\n",
    "filtered_df = data_df.filter(data_df[\"class_index\"] == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "206b21e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data:\n",
      "+-----------+--------------------+--------------------+\n",
      "|class_index|        review_title|         review_text|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          5|           Inspiring|I hope a lot of p...|\n",
      "|          5|The best soundtra...|I'm reading a lot...|\n",
      "|          5| Too good to be true|Probably the grea...|\n",
      "|          5|There's a reason ...|There's a reason ...|\n",
      "|          5|Lower leg comfort...|Excellent stockin...|\n",
      "|          5|CHECK OUT THE K60...|Greetings. Kodak ...|\n",
      "|          5|                ahhh|These will more t...|\n",
      "|          5|   Excellent Product|This is an excell...|\n",
      "|          5|\"To \"\"Disappointe...|\"You said \"\"...bu...|\n",
      "|          5|Awesome - wanna u...|It's just awesome...|\n",
      "|          5|\"Very Happy - Has...|\"I got this machi...|\n",
      "|          5|This is a very go...|Many useful conce...|\n",
      "|          5|Great reference b...|It seems somebody...|\n",
      "|          5|Here is a little ...|After you watch a...|\n",
      "|          5|Rochelle explains...|Wondering what th...|\n",
      "|          5|Spreading a Passi...|\"Piper has a pass...|\n",
      "|          5|supermarionation ...|if you love thund...|\n",
      "|          5|   Excellent Service|Recieved this ite...|\n",
      "|          5|    Dk Travel guides|The Dk Travel gui...|\n",
      "|          5|creativity for ev...|did you know this...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print filtered data\n",
    "print(\"Filtered Data:\")\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4ed398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|class_index|        review_title|         review_text|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          3|  more like funchuck|\"Gave this to my ...|\n",
      "|          5|           Inspiring|I hope a lot of p...|\n",
      "|          5|The best soundtra...|I'm reading a lot...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b429827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+\n",
      "|class_index|review_title|review_text|\n",
      "+-----------+------------+-----------+\n",
      "|    3000000|     3000000|    3000000|\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = data_df.agg(*[count(col(c).isNull().cast(\"int\")).alias(c) for c in data_df.columns])\n",
    "\n",
    "# Show the null counts\n",
    "null_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71d1d8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|class_index|        review_title|         review_text|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          3|  more like funchuck|\"Gave this to my ...|\n",
      "|          5|           Inspiring|I hope a lot of p...|\n",
      "|          5|The best soundtra...|I'm reading a lot...|\n",
      "|          4|    Chrono Cross OST|\"The music of Yas...|\n",
      "|          5| Too good to be true|Probably the grea...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show a sample of the DataFrame\n",
    "data_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2515f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with empty review_title: 0\n",
      "Number of rows with empty review_text: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter for rows where review_title or review_text is empty\n",
    "empty_title_df = data_df.filter(col(\"review_title\") == \"\")\n",
    "empty_text_df = data_df.filter(col(\"review_text\") == \"\")\n",
    "\n",
    "# Count the number of rows with empty review_title and review_text\n",
    "empty_title_count = empty_title_df.count()\n",
    "empty_text_count = empty_text_df.count()\n",
    "\n",
    "print(\"Number of rows with empty review_title:\", empty_title_count)\n",
    "print(\"Number of rows with empty review_text:\", empty_text_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "feb08756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+\n",
      "|class_index|        review_title|         review_text|                text|\n",
      "+-----------+--------------------+--------------------+--------------------+\n",
      "|          3|  more like funchuck|\"Gave this to my ...|more like funchuc...|\n",
      "|          5|           Inspiring|I hope a lot of p...|Inspiring I hope ...|\n",
      "|          5|The best soundtra...|I'm reading a lot...|The best soundtra...|\n",
      "|          4|    Chrono Cross OST|\"The music of Yas...|Chrono Cross OST ...|\n",
      "|          5| Too good to be true|Probably the grea...|Too good to be tr...|\n",
      "|          5|There's a reason ...|There's a reason ...|There's a reason ...|\n",
      "|          1|        Buyer beware|\"This is a self-p...|Buyer beware \"Thi...|\n",
      "|          4|Errors, but great...|I was a dissapoin...|Errors, but great...|\n",
      "|          1|          The Worst!|A complete waste ...|The Worst! A comp...|\n",
      "|          1|           Oh please|I guess you have ...|Oh please I guess...|\n",
      "|          1|Awful beyond belief!|\"I feel I have to...|Awful beyond beli...|\n",
      "|          4|A romantic zen ba...|\"When you hear fo...|A romantic zen ba...|\n",
      "|          5|Lower leg comfort...|Excellent stockin...|Lower leg comfort...|\n",
      "|          3|Delivery was very...|It took almost 3 ...|Delivery was very...|\n",
      "|          2|sizes recomended ...|sizes are much sm...|sizes recomended ...|\n",
      "|          3|            Overbury|Full of intrigue ...|Overbury Full of ...|\n",
      "|          1|Another Abysmal D...|\"Rather than scra...|Another Abysmal D...|\n",
      "|          4|Wardell's book is...|\"Steven Wardell's...|Wardell's book is...|\n",
      "|          4|i liked this albu...|\"I heard a song o...|i liked this albu...|\n",
      "|          3|Better than I tho...|I wrote a harsh r...|Better than I tho...|\n",
      "+-----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Concatenate review_title and review_text into a new column named text\n",
    "data_df = data_df.withColumn('text', concat_ws(' ', data_df['review_title'], data_df['review_text']))\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6d57e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+\n",
      "|class_index|        review_title|         review_text|                text|\n",
      "+-----------+--------------------+--------------------+--------------------+\n",
      "|          3|  more like funchuck|\"Gave this to my ...|more like funchuc...|\n",
      "|          5|           Inspiring|I hope a lot of p...|Inspiring I hope ...|\n",
      "|          5|The best soundtra...|I'm reading a lot...|The best soundtra...|\n",
      "|          4|    Chrono Cross OST|\"The music of Yas...|Chrono Cross OST ...|\n",
      "|          5| Too good to be true|Probably the grea...|Too good to be tr...|\n",
      "+-----------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, concat_ws, col\n",
    "\n",
    "# Concatenate review_title and review_text into a new column named text\n",
    "data_df = data_df.withColumn('text', concat_ws(' ', col('review_title'), col('review_text')))\n",
    "\n",
    "data_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b46c07c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|class_index|                text|\n",
      "+-----------+--------------------+\n",
      "|          3|more like funchuc...|\n",
      "|          5|Inspiring I hope ...|\n",
      "|          5|The best soundtra...|\n",
      "|          4|Chrono Cross OST ...|\n",
      "|          5|Too good to be tr...|\n",
      "|          5|There's a reason ...|\n",
      "|          1|Buyer beware \"Thi...|\n",
      "|          4|Errors, but great...|\n",
      "|          1|The Worst! A comp...|\n",
      "|          1|Oh please I guess...|\n",
      "|          1|Awful beyond beli...|\n",
      "|          4|A romantic zen ba...|\n",
      "|          5|Lower leg comfort...|\n",
      "|          3|Delivery was very...|\n",
      "|          2|sizes recomended ...|\n",
      "|          3|Overbury Full of ...|\n",
      "|          1|Another Abysmal D...|\n",
      "|          4|Wardell's book is...|\n",
      "|          4|i liked this albu...|\n",
      "|          3|Better than I tho...|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Drop the review_title and review_text columns\n",
    "data_df = data_df.drop(col('review_title')).drop(col('review_text'))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "data_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d975c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenize_udf = udf(lambda text: word_tokenize(text), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "448e69d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|class_index|                text|              tokens|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          3|more like funchuc...|[more, like, func...|\n",
      "|          5|Inspiring I hope ...|[Inspiring, I, ho...|\n",
      "|          5|The best soundtra...|[The, best, sound...|\n",
      "|          4|Chrono Cross OST ...|[Chrono, Cross, O...|\n",
      "|          5|Too good to be tr...|[Too, good, to, b...|\n",
      "|          5|There's a reason ...|[There, 's, a, re...|\n",
      "|          1|Buyer beware \"Thi...|[Buyer, beware, `...|\n",
      "|          4|Errors, but great...|[Errors, ,, but, ...|\n",
      "|          1|The Worst! A comp...|[The, Worst, !, A...|\n",
      "|          1|Oh please I guess...|[Oh, please, I, g...|\n",
      "|          1|Awful beyond beli...|[Awful, beyond, b...|\n",
      "|          4|A romantic zen ba...|[A, romantic, zen...|\n",
      "|          5|Lower leg comfort...|[Lower, leg, comf...|\n",
      "|          3|Delivery was very...|[Delivery, was, v...|\n",
      "|          2|sizes recomended ...|[sizes, recomende...|\n",
      "|          3|Overbury Full of ...|[Overbury, Full, ...|\n",
      "|          1|Another Abysmal D...|[Another, Abysmal...|\n",
      "|          4|Wardell's book is...|[Wardell, 's, boo...|\n",
      "|          4|i liked this albu...|[i, liked, this, ...|\n",
      "|          3|Better than I tho...|[Better, than, I,...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenized_df = data_df.withColumn(\"tokens\", tokenize_udf(col(\"text\")))\n",
    "tokenized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f0f6901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "remove_stopwords_udf = udf(lambda tokens: [token for token in tokens if token not in stop_words], ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34abb546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+\n",
      "|class_index|                text|              tokens|     filtered_tokens|\n",
      "+-----------+--------------------+--------------------+--------------------+\n",
      "|          3|more like funchuc...|[more, like, func...|[like, funchuck, ...|\n",
      "|          5|Inspiring I hope ...|[Inspiring, I, ho...|[Inspiring, I, ho...|\n",
      "|          5|The best soundtra...|[The, best, sound...|[The, best, sound...|\n",
      "|          4|Chrono Cross OST ...|[Chrono, Cross, O...|[Chrono, Cross, O...|\n",
      "|          5|Too good to be tr...|[Too, good, to, b...|[Too, good, true,...|\n",
      "|          5|There's a reason ...|[There, 's, a, re...|[There, 's, reaso...|\n",
      "|          1|Buyer beware \"Thi...|[Buyer, beware, `...|[Buyer, beware, `...|\n",
      "|          4|Errors, but great...|[Errors, ,, but, ...|[Errors, ,, great...|\n",
      "|          1|The Worst! A comp...|[The, Worst, !, A...|[The, Worst, !, A...|\n",
      "|          1|Oh please I guess...|[Oh, please, I, g...|[Oh, please, I, g...|\n",
      "|          1|Awful beyond beli...|[Awful, beyond, b...|[Awful, beyond, b...|\n",
      "|          4|A romantic zen ba...|[A, romantic, zen...|[A, romantic, zen...|\n",
      "|          5|Lower leg comfort...|[Lower, leg, comf...|[Lower, leg, comf...|\n",
      "|          3|Delivery was very...|[Delivery, was, v...|[Delivery, long, ...|\n",
      "|          2|sizes recomended ...|[sizes, recomende...|[sizes, recomende...|\n",
      "|          3|Overbury Full of ...|[Overbury, Full, ...|[Overbury, Full, ...|\n",
      "|          1|Another Abysmal D...|[Another, Abysmal...|[Another, Abysmal...|\n",
      "|          4|Wardell's book is...|[Wardell, 's, boo...|[Wardell, 's, boo...|\n",
      "|          4|i liked this albu...|[i, liked, this, ...|[liked, album, th...|\n",
      "|          3|Better than I tho...|[Better, than, I,...|[Better, I, thoug...|\n",
      "+-----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df = tokenized_df.withColumn(\"filtered_tokens\", remove_stopwords_udf(col(\"tokens\")))\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9518b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis using NLTK\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_udf = udf(lambda text: sia.polarity_scores(text)[\"compound\"], DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46aa7c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+---------------+\n",
      "|class_index|                text|              tokens|     filtered_tokens|sentiment_score|\n",
      "+-----------+--------------------+--------------------+--------------------+---------------+\n",
      "|          3|more like funchuc...|[more, like, func...|[like, funchuck, ...|         0.5095|\n",
      "|          5|Inspiring I hope ...|[Inspiring, I, ho...|[Inspiring, I, ho...|         0.9822|\n",
      "|          5|The best soundtra...|[The, best, sound...|[The, best, sound...|         0.8957|\n",
      "|          4|Chrono Cross OST ...|[Chrono, Cross, O...|[Chrono, Cross, O...|          0.926|\n",
      "|          5|Too good to be tr...|[Too, good, to, b...|[Too, good, true,...|         0.9807|\n",
      "|          5|There's a reason ...|[There, 's, a, re...|[There, 's, reaso...|         0.6369|\n",
      "|          1|Buyer beware \"Thi...|[Buyer, beware, `...|[Buyer, beware, `...|        -0.6479|\n",
      "|          4|Errors, but great...|[Errors, ,, but, ...|[Errors, ,, great...|         0.8537|\n",
      "|          1|The Worst! A comp...|[The, Worst, !, A...|[The, Worst, !, A...|        -0.9565|\n",
      "|          1|Oh please I guess...|[Oh, please, I, g...|[Oh, please, I, g...|         0.9097|\n",
      "|          1|Awful beyond beli...|[Awful, beyond, b...|[Awful, beyond, b...|        -0.9552|\n",
      "|          4|A romantic zen ba...|[A, romantic, zen...|[A, romantic, zen...|         0.6891|\n",
      "|          5|Lower leg comfort...|[Lower, leg, comf...|[Lower, leg, comf...|         0.4245|\n",
      "|          3|Delivery was very...|[Delivery, was, v...|[Delivery, long, ...|        -0.0229|\n",
      "|          2|sizes recomended ...|[sizes, recomende...|[sizes, recomende...|         0.4926|\n",
      "|          3|Overbury Full of ...|[Overbury, Full, ...|[Overbury, Full, ...|         0.7003|\n",
      "|          1|Another Abysmal D...|[Another, Abysmal...|[Another, Abysmal...|         0.9634|\n",
      "|          4|Wardell's book is...|[Wardell, 's, boo...|[Wardell, 's, boo...|         0.9612|\n",
      "|          4|i liked this albu...|[i, liked, this, ...|[liked, album, th...|         0.6486|\n",
      "|          3|Better than I tho...|[Better, than, I,...|[Better, I, thoug...|         0.9249|\n",
      "+-----------+--------------------+--------------------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_df = filtered_df.withColumn(\"sentiment_score\", sentiment_udf(col(\"text\")))\n",
    "sentiment_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4599398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "tokenized_df = tokenizer.transform(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbbc0dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "filtered_df = remover.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76a9501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the filtered tokens\n",
    "exploded_df = filtered_df.withColumn(\"token\", explode(\"filtered_tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e473da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+---------------+\n",
      "|class_index|                text|              tokens|     filtered_tokens|sentiment_score|\n",
      "+-----------+--------------------+--------------------+--------------------+---------------+\n",
      "|          3|more like funchuc...|[more, like, func...|[like, funchuck, ...|         0.5095|\n",
      "|          5|Inspiring I hope ...|[Inspiring, I, ho...|[Inspiring, I, ho...|         0.9822|\n",
      "|          5|The best soundtra...|[The, best, sound...|[The, best, sound...|         0.8957|\n",
      "+-----------+--------------------+--------------------+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fc49aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 3440123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute vocabulary size\n",
    "vocabulary_size = exploded_df.select(\"token\").distinct().count()\n",
    "print(\"Vocabulary Size:\", vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d9758ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------                                        \n",
      "Exception occurred during processing of request from ('127.0.0.1', 48570)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hduser/anaconda3/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/hduser/anaconda3/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/hduser/anaconda3/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/hduser/anaconda3/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "       ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "X = sentiment_df.select(\"sentiment_score\").toPandas()\n",
    "y = sentiment_df.select(\"class_index\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "298221f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e447eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrames to NumPy arrays and ensure correct data types\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('int32')  # Ensure integer labels\n",
    "y_test = y_test.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae862887",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 1000  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2a0b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[X_train < vocabulary_size]\n",
    "X_test = X_test[X_test < vocabulary_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5095d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Set GPU memory growth to avoid allocating all GPU memory upfront\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87553e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=128))  \n",
    "model.add(Conv1D(filters=128, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=1))\n",
    "model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e12c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e5cf047",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b33f2bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30  # Adjust as needed\n",
    "\n",
    "# Optimize batch size\n",
    "batch_size = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b1480f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1ms/step - accuracy: 0.2003 - loss: -737936920281088.0000 - val_accuracy: 0.2004 - val_loss: -19344659402719232.0000\n",
      "Epoch 2/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 1ms/step - accuracy: 0.1994 - loss: -63422648188665856.0000 - val_accuracy: 0.2004 - val_loss: -392202807014326272.0000\n",
      "Epoch 3/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1ms/step - accuracy: 0.2005 - loss: -719899603289243648.0000 - val_accuracy: 0.2004 - val_loss: -2444951296214564864.0000\n",
      "Epoch 4/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1ms/step - accuracy: 0.1999 - loss: -3673536792998445056.0000 - val_accuracy: 0.2004 - val_loss: -9195827621311545344.0000\n",
      "Epoch 5/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1ms/step - accuracy: 0.1996 - loss: -12490076058558660608.0000 - val_accuracy: 0.2004 - val_loss: -26039406226153930752.0000\n",
      "Epoch 6/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1ms/step - accuracy: 0.2004 - loss: -33251969407899402240.0000 - val_accuracy: 0.2004 - val_loss: -61419832233334669312.0000\n",
      "Epoch 7/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1ms/step - accuracy: 0.1999 - loss: -75339908925722984448.0000 - val_accuracy: 0.2004 - val_loss: -127516406389088976896.0000\n",
      "Epoch 8/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1ms/step - accuracy: 0.1997 - loss: -152051603742631395328.0000 - val_accuracy: 0.2004 - val_loss: -240849464623588442112.0000\n",
      "Epoch 9/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1ms/step - accuracy: 0.2000 - loss: -280884019997051977728.0000 - val_accuracy: 0.2004 - val_loss: -423080322787927130112.0000\n",
      "Epoch 10/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1ms/step - accuracy: 0.2000 - loss: -484843532502616244224.0000 - val_accuracy: 0.2004 - val_loss: -701523391786944299008.0000\n",
      "Epoch 11/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 1ms/step - accuracy: 0.2001 - loss: -792690884943712092160.0000 - val_accuracy: 0.2004 - val_loss: -1109946468112617439232.0000\n",
      "Epoch 12/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1ms/step - accuracy: 0.2005 - loss: -1240465288913441783808.0000 - val_accuracy: 0.2004 - val_loss: -1689045063160284839936.0000\n",
      "Epoch 13/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1ms/step - accuracy: 0.2005 - loss: -1869709214012065972224.0000 - val_accuracy: 0.2004 - val_loss: -2487434338638759985152.0000\n",
      "Epoch 14/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1ms/step - accuracy: 0.2003 - loss: -2735653891275982110720.0000 - val_accuracy: 0.2004 - val_loss: -3561366646431202410496.0000\n",
      "Epoch 15/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1ms/step - accuracy: 0.1999 - loss: -3886618300369759895552.0000 - val_accuracy: 0.2004 - val_loss: -4977324264974343733248.0000\n",
      "Epoch 16/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2ms/step - accuracy: 0.2001 - loss: -5405299085763548282880.0000 - val_accuracy: 0.2004 - val_loss: -6810464390749649633280.0000\n",
      "Epoch 17/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1ms/step - accuracy: 0.1997 - loss: -7359193102447632449536.0000 - val_accuracy: 0.2004 - val_loss: -9145396712906483040256.0000\n",
      "Epoch 18/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1ms/step - accuracy: 0.1999 - loss: -9833795559848107048960.0000 - val_accuracy: 0.2004 - val_loss: -12077792887178935664640.0000\n",
      "Epoch 19/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1ms/step - accuracy: 0.1999 - loss: -12920873492822133571584.0000 - val_accuracy: 0.2004 - val_loss: -15718579387138909929472.0000\n",
      "Epoch 20/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1ms/step - accuracy: 0.2002 - loss: -16766902538600263450624.0000 - val_accuracy: 0.2004 - val_loss: -20184743968506795524096.0000\n",
      "Epoch 21/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1ms/step - accuracy: 0.1999 - loss: -21472176734984130789376.0000 - val_accuracy: 0.2004 - val_loss: -25613069959165089153024.0000\n",
      "Epoch 22/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2ms/step - accuracy: 0.2000 - loss: -27188561965008296083456.0000 - val_accuracy: 0.2004 - val_loss: -32148428026027138088960.0000\n",
      "Epoch 23/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1ms/step - accuracy: 0.1994 - loss: -34012724616974546501632.0000 - val_accuracy: 0.2004 - val_loss: -39953076108267660247040.0000\n",
      "Epoch 24/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2ms/step - accuracy: 0.1995 - loss: -42178277662553658097664.0000 - val_accuracy: 0.2004 - val_loss: -49203204030508644171776.0000\n",
      "Epoch 25/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1ms/step - accuracy: 0.1996 - loss: -51836057912265337208832.0000 - val_accuracy: 0.2004 - val_loss: -60086963177982374445056.0000\n",
      "Epoch 26/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1ms/step - accuracy: 0.1998 - loss: -63145150519346819235840.0000 - val_accuracy: 0.2004 - val_loss: -72815504819218116771840.0000\n",
      "Epoch 27/30\n",
      "\u001b[1m  104/37500\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 1ms/step - accuracy: 0.1999 - loss: -73101546445950926454784.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 20:05:36.683647: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 19200000 bytes after encountering the first element of size 19200000 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 1ms/step - accuracy: 0.2004 - loss: -76330555321181534158848.0000 - val_accuracy: 0.2004 - val_loss: -87596769156210821693440.0000\n",
      "Epoch 28/30\n",
      "\u001b[1m  105/37500\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 1ms/step - accuracy: 0.2002 - loss: -87135573532770318680064.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 20:06:32.026540: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 19200000 bytes after encountering the first element of size 19200000 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 1ms/step - accuracy: 0.1997 - loss: -91692964154090127884288.0000 - val_accuracy: 0.2004 - val_loss: -104564386997103826567168.0000\n",
      "Epoch 29/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 1ms/step - accuracy: 0.1998 - loss: -109270711658100778074112.0000 - val_accuracy: 0.2004 - val_loss: -124054876313253509595136.0000\n",
      "Epoch 30/30\n",
      "\u001b[1m37500/37500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1ms/step - accuracy: 0.1993 - loss: -129496071339847014416384.0000 - val_accuracy: 0.2004 - val_loss: -146352351190752165888000.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0xffff11353e90>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5675018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18750/18750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 360us/step - accuracy: 0.2005 - loss: -146261918910234566328320.0000\n",
      "Test Loss: -1.4634928874300555e+23\n",
      "Test Accuracy: 0.20043833553791046\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "139b5f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18750/18750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 366us/step - accuracy: 0.2005 - loss: -146261918910234566328320.0000\n",
      "Test Loss: -1.4634928874300555e+23\n",
      "Test Accuracy: 0.20043833553791046\n",
      "\u001b[1m18750/18750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 363us/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Confusion Matrix:\n",
      "[[120263      0      0      0      0]\n",
      " [119812      0      0      0      0]\n",
      " [119896      0      0      0      0]\n",
      " [119849      0      0      0      0]\n",
      " [120180      0      0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Print out model predictions\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Predictions:\", y_pred)\n",
    "\n",
    "# Print out confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcba248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
